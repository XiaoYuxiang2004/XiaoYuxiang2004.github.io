## **§ 5.5 充分统计量**

### **5.5.1 充分性的概念**

在统计学中，我们经常需要对样本数据进行处理，以提取关键信息，从而对总体参数进行推断。这个处理过程可以形象地比喻为“去粗取精，简化样本”。我们希望通过这种处理，能够得到一个**统计量**，它能够有效地**简化样本信息**，同时又**不损失任何与我们感兴趣的总体参数相关的信息**。如果一个统计量能达到这个目标，我们就称它为**充分统计量**。

用更严谨的语言来说，一个统计量 $T$ 被称为是关于参数 $\theta$ 的**充分统计量**，如果当已知 $T$ 的值时，样本 $X = (X_1, X_2, \dots, X_n)$ 的条件分布不再依赖于 $\theta$。这意味着，一旦我们知道了充分统计量 $T$ 的值，样本中所有关于参数 $\theta$ 的信息就已经被 $T$ 捕捉到了，样本的其余部分（在给定 $T$ 值的前提下）对于估计或推断 $\theta$ 来说，不再提供任何额外的信息。

**费希尔 (R. A. Fisher)** 对充分性给出了经典的定义：

> **定义 5.5.1 (充分性的概念)**
> 设 $X_1, X_2, \dots, X_n$ 是来自总体，其分布由参数 $\theta$ 确定。称统计量 $T(X_1, X_2, \dots, X_n)$ 为 $\theta$ 的**充分统计量**，如果对任意给定的 $t$，在已知 $T(X_1, X_2, \dots, X_n) = t$ 的条件下，样本 $X_1, X_2, \dots, X_n$ 的条件分布与 $\theta$ 无关。

用数学公式表示就是：
$P(X_1=x_1, X_2=x_2, \dots, X_n=x_n | T(X_1, X_2, \dots, X_n)=t, \theta)$
或者对于连续型随机变量，是条件概率密度函数：
$f(x_1, x_2, \dots, x_n | T(X_1, X_2, \dots, X_n)=t, \theta)$
不依赖于 $\theta$。

**例 5.5.1 (伯努利试验的充分统计量)**
假设我们进行 $n$ 次独立的伯努利试验，每次试验成功的概率为 $p$（未知参数）。令 $X_i=1$ 表示第 $i$ 次试验成功， $X_i=0$ 表示失败。我们感兴趣的统计量是成功次数 $S = \sum_{i=1}^n X_i$。我们要证明 $S$ 是 $p$ 的充分统计量。

首先，明确 $X_i$ 的概率质量函数 (PMF) 为 $P(X_i=x_i) = p^{x_i} (1-p)^{1-x_i}$。
则样本的联合 PMF 为：
$$P(X_1=x_1, \dots, X_n=x_n | p) = \prod_{i=1}^n p^{x_i} (1-p)^{1-x_i} = p^{\sum x_i} (1-p)^{n-\sum x_i}$$
由于 $S = \sum X_i$，因此样本的联合 PMF 可以写为 $p^s (1-p)^{n-s}$，其中 $s$ 是样本中成功的次数。
同时，我们知道 $S$ 服从二项分布 $B(n, p)$，其 PMF 为：
$$P(S=s | p) = \binom{n}{s} p^s (1-p)^{n-s}$$
根据条件概率的定义，我们计算 $P(X_1=x_1, \dots, X_n=x_n | S=s, p)$：
$$P(X_1=x_1, \dots, X_n=x_n | S=s, p) = \frac{P(X_1=x_1, \dots, X_n=x_n, S=s | p)}{P(S=s | p)}$$
由于如果 $(x_1, \dots, x_n)$ 满足 $\sum x_i = s$，那么 $S=s$ 这个事件就已经包含在样本 $(X_1, \dots, X_n)$ 的事件中了，所以分子 $P(X_1=x_1, \dots, X_n=x_n, S=s | p)$ 实际上就是 $P(X_1=x_1, \dots, X_n=x_n | p)$。
因此：
$$P(X_1=x_1, \dots, X_n=x_n | S=s, p) = \frac{p^s (1-p)^{n-s}}{\binom{n}{s} p^s (1-p)^{n-s}} = \frac{1}{\binom{n}{s}}$$
这个结果 $\frac{1}{\binom{n}{s}}$ **不依赖于参数 $p$**。这说明一旦我们知道总共发生了多少次成功（即 $S$ 的值），那么这些成功具体发生在哪些次试验中（即 $X_i$ 的具体排列）对于推断 $p$ 而言，就没有任何额外的信息了。因此，**成功次数 $S$ 是参数 $p$ 的充分统计量**。

### **因子分解定理 (Fisher-Neyman Factorization Theorem)**

直接使用定义来验证充分性通常比较复杂，尤其是在处理连续型随机变量时。因此，**因子分解定理**提供了一个更实用、更方便的判别方法。

> **定理 5.5.1 (因子分解定理)**
> 设 $X_1, X_2, \dots, X_n$ 是来自某个总体，其概率密度函数 (PDF) 或概率质量函数 (PMF) 为 $p(x_1, x_2, \dots, x_n; \theta)$。统计量 $T(X_1, X_2, \dots, X_n)$ 是参数 $\theta$ 的充分统计量，当且仅当存在两个非负函数 $g$ 和 $h$，使得样本的联合 PDF/PMF 可以被分解为以下形式：
> $$p(x_1, x_2, \dots, x_n; \theta) = g(T(x_1, x_2, \dots, x_n), \theta) h(x_1, x_2, \dots, x_n)$$
> 其中：
>
> 1.  函数 $g$ 依赖于参数 $\theta$ **仅仅通过**统计量 $T(x_1, x_2, \dots, x_n)$。
> 2.  函数 $h$ **不依赖于**参数 $\theta$。
>

**证明思路 (简要)**
*   **必要性 (充分 $\implies$ 可分解)：** 如果 $T$ 是充分统计量，那么 $p(x; \theta) = P(X=x | T(X)=t, \theta) P(T(X)=t | \theta)$。由于 $T$ 是充分的， $P(X=x | T(X)=t, \theta)$ 不依赖于 $\theta$，我们可以将其设为 $h(x_1, \dots, x_n)$。而 $P(T(X)=t | \theta)$ 显然是 $T(x_1, \dots, x_n)$ 和 $\theta$ 的函数，可以设为 $g(T(x_1, \dots, x_n), \theta)$。从而得到分解形式。
*   **充分性 (可分解 $\implies$ 充分)：** 如果 $p(x; \theta) = g(T(x), \theta) h(x)$，那么 $P(T(X)=t | \theta) = \sum_{x: T(x)=t} p(x; \theta) = \sum_{x: T(x)=t} g(t, \theta) h(x) = g(t, \theta) \sum_{x: T(x)=t} h(x)$ (对于离散情况，连续情况是积分)。
    于是，条件概率为：
    $$P(X=x | T(X)=t, \theta) = \frac{p(x; \theta)}{P(T(X)=t | \theta)} = \frac{g(t, \theta) h(x)}{g(t, \theta) \sum_{x: T(x)=t} h(x)} = \frac{h(x)}{\sum_{x: T(x)=t} h(x)}$$
    这个结果显然不依赖于 $\theta$，因此 $T$ 是充分统计量。

**例 5.5.4 (均匀分布的充分统计量)**
设 $X_1, X_2, \dots, X_n$ 是来自均匀分布 $U(0, \theta)$ 的独立同分布样本，其中 $\theta > 0$ 是未知参数。
每个 $X_i$ 的 PDF 为 $f(x_i; \theta) = \frac{1}{\theta}$ 当 $0 < x_i < \theta$ 时，否则为 $0$。
样本的联合 PDF 为：
$$p(x_1, x_2, \dots, x_n; \theta) = \prod_{i=1}^n f(x_i; \theta) = \left(\frac{1}{\theta}\right)^n \quad \text{当 } 0 < x_i < \theta \text{ 对所有 } i \text{ 成立时}$$
为了更清晰地表达条件，我们可以使用指示函数 $I(condition)$。
$I(condition) = 1$ 如果条件成立，否则为 $0$。
因此，联合 PDF 可以写为：
$$p(x_1, x_2, \dots, x_n; \theta) = \left(\frac{1}{\theta}\right)^n \cdot I(0 < x_1 < \theta, \dots, 0 < x_n < \theta)$$
这等价于 $0 < \min(x_i)$ 且 $\max(x_i) < \theta$。
所以，我们可以写成：
$$p(x_1, x_2, \dots, x_n; \theta) = \left(\frac{1}{\theta}\right)^n \cdot I(\max(x_1, \dots, x_n) < \theta) \cdot I(\min(x_1, \dots, x_n) > 0)$$
现在，我们将它分解为 $g(T(x), \theta) h(x)$ 的形式。
令 $T(X_1, \dots, X_n) = \max(X_1, \dots, X_n)$。
我们取：
$g(T(x), \theta) = \left(\frac{1}{\theta}\right)^n \cdot I(T(x) < \theta)$
$h(x_1, \dots, x_n) = I(\min(x_1, \dots, x_n) > 0)$
这里，$h(x_1, \dots, x_n)$ 显然不依赖于 $\theta$，而 $g(T(x), \theta)$ 仅仅通过 $T(x)$ 依赖于 $\theta$。
根据因子分解定理，**$T(X_1, \dots, X_n) = \max(X_1, \dots, X_n)$ 是 $\theta$ 的充分统计量**。

**例 5.5.5 (正态分布的充分统计量)**
设 $X_1, X_2, \dots, X_n$ 是来自正态分布 $N(\mu, \sigma^2)$ 的独立同分布样本。

**情况一：已知 $\sigma^2$， $\mu$ 未知。**
每个 $X_i$ 的 PDF 为 $f(x_i; \mu) = \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$。
样本的联合 PDF 为：
$$p(x_1, \dots, x_n; \mu) = \prod_{i=1}^n \frac{1}{\sqrt{2\pi}\sigma} \exp\left(-\frac{(x_i-\mu)^2}{2\sigma^2}\right)$$
$$p(x_1, \dots, x_n; \mu) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2\right)$$
展开和式 $\sum (x_i-\mu)^2 = \sum (x_i^2 - 2x_i\mu + \mu^2) = \sum x_i^2 - 2\mu \sum x_i + n\mu^2$。
$$p(x_1, \dots, x_n; \mu) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{1}{2\sigma^2} \left(\sum x_i^2 - 2n\mu\bar{x} + n\mu^2\right)\right)$$
我们可以将上式分解：
$$p(x_1, \dots, x_n; \mu) = \left[ \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{n\mu^2}{2\sigma^2} + \frac{n\mu\bar{x}}{\sigma^2}\right) \right] \cdot \left[ \exp\left(-\frac{\sum x_i^2}{2\sigma^2}\right) \right]$$
令 $T(X) = \bar{X} = \frac{1}{n}\sum X_i$。
则 $g(T(x), \mu) = \left(\frac{1}{\sqrt{2\pi}\sigma}\right)^n \exp\left(-\frac{n\mu^2}{2\sigma^2} + \frac{n\mu\bar{x}}{\sigma^2}\right)$，它依赖于 $\mu$ 和 $\bar{x}$。
而 $h(x) = \exp\left(-\frac{\sum x_i^2}{2\sigma^2}\right)$。请注意，这里的 $h(x)$ 仍然依赖于 $\sum x_i^2$。为了使其完全不依赖于 $\mu$，我们必须确保 $h(x)$ 不包含 $\mu$。这个分解是符合要求的，因为 $h(x)$ 不含 $\mu$。
因此，**样本均值 $\bar{X}$ 是参数 $\mu$ 的充分统计量**（当 $\sigma^2$ 已知时）。

**情况二： $\mu$ 和 $\sigma^2$ 都未知。**
此时，参数是向量 $(\mu, \sigma^2)$。
样本的联合 PDF 依然是：
$$p(x_1, \dots, x_n; \mu, \sigma^2) = \left(\frac{1}{2\pi\sigma^2}\right)^{n/2} \exp\left(-\frac{1}{2\sigma^2} \sum_{i=1}^n (x_i-\mu)^2\right)$$
我们知道 $\sum (x_i-\mu)^2 = \sum (x_i-\bar{x} + \bar{x}-\mu)^2 = \sum (x_i-\bar{x})^2 + n(\bar{x}-\mu)^2$。
同时，样本方差 $S^2 = \frac{1}{n-1}\sum (x_i-\bar{x})^2$，所以 $\sum (x_i-\bar{x})^2 = (n-1)S^2$。
代入联合 PDF：
$$p(x_1, \dots, x_n; \mu, \sigma^2) = \left(\frac{1}{2\pi\sigma^2}\right)^{n/2} \exp\left(-\frac{1}{2\sigma^2} \left[ n(\bar{x}-\mu)^2 + (n-1)s^2 \right]\right)$$
令 $T(X) = (\bar{X}, S^2)$。
我们可以将上式视为 $g((\bar{x}, s^2), (\mu, \sigma^2))$ 乘以 $h(x_1, \dots, x_n)=1$。
因此，**样本均值 $\bar{X}$ 和样本方差 $S^2$ 组成的统计量对 $(\mu, \sigma^2)$ 是充分的**。

### **充分统计量的函数**

> **定理 5.5.2**
> 若统计量 $T$ 是参数 $\theta$ 的充分统计量，并且存在某个函数 $h(\cdot)$，使得统计量 $S = h(T)$，且 $T$ 可以表示为 $S$ 的函数（即 $T=h^{-1}(S)$，如果 $h$ 是单射），则 $S$ 也是 $\theta$ 的充分统计量。

**证明**
由于 $T$ 是 $\theta$ 的充分统计量，根据因子分解定理，样本的联合 PDF/PMF 可以表示为：
$$p(x_1, x_2, \dots, x_n; \theta) = g(T(x_1, x_2, \dots, x_n), \theta) h_0(x_1, x_2, \dots, x_n)$$
其中 $h_0$ 不依赖于 $\theta$。
现在，如果存在函数 $h$ 使得 $S = h(T)$，并且 $T$ 可以表示为 $S$ 的函数，即 $T = h^{-1}(S)$ (例如 $h$ 是一个单射函数)，我们可以将 $T$ 替换为 $h^{-1}(S)$：
$$p(x_1, x_2, \dots, x_n; \theta) = g(h^{-1}(S(x_1, x_2, \dots, x_n)), \theta) h_0(x_1, x_2, \dots, x_n)$$
令 $g^*(S(x_1, x_2, \dots, x_n), \theta) = g(h^{-1}(S(x_1, x_2, \dots, x_n)), \theta)$。
那么上式变为：
$$p(x_1, x_2, \dots, x_n; \theta) = g^*(S(x_1, x_2, \dots, x_n), \theta) h_0(x_1, x_2, \dots, x_n)$$
这仍然符合因子分解定理的形式，其中 $g^*$ 依赖于 $\theta$ 只通过 $S$，而 $h_0$ 不依赖于 $\theta$。
因此，**$S$ 也是 $\theta$ 的充分统计量**。